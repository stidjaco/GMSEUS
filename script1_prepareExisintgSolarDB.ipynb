{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Solar Energy Array and Panel Databases\n",
    "\n",
    "**Script Notes**\n",
    "* Calls and preprocesses existing spatiotemporal solar pv databases. \n",
    "* This is a multi-step process:\n",
    "    * Process and remove duplicates for existing array polygon shapefiles from USPVDB, CCVPV, CWSD, OSM, and SAM datasets. Export as existing array shapes\n",
    "    * Process and remove duplicates for centroid shapefiles from InSPIRE, LBNL-USS PV-DAQ, and SolarSPACES, GSPT, and GPPDB. Export as need to digitize centroids, and run `script2_digitizeSolarArrays` GEE script.\n",
    "\n",
    "## Array Polygon-Level Data\n",
    "\n",
    "**United States Solar Photovoltaic Database (USPVDB)**\n",
    "* Downloaded from [USPVDB Portal](https://eerscmap.usgs.gov/uspvdb/data/)\n",
    "* Last Download: 10-11-2024 (Up-to-date as of 12-11-2024)\n",
    "* Version 2.0\n",
    "\n",
    "**California's Central Valley Photovoltaic Dataset (CCVPV) Arrays and Panels**\n",
    "* Downloaded from [figshare](https://doi.org/10.6084/m9.figshare.23629326.v1)\n",
    "* Last Download: 07-18-2024 (Up-to-date as of 12-11-2024)\n",
    "* Version 1.0\n",
    "\n",
    "**Chesapeake Watershed Solar Data (CWSD) Arrays**\n",
    "* Downloaded from [OSFHOME](https://osf.io/vq7mt/)\n",
    "* Last Download: 12-01-2024 (Up-to-date as of 12-11-2024)\n",
    "* We downloaded derived polygons as well as manually annotated training polygons, and preferenced training polygons over derived for their completeness and quality\n",
    "* No Version details\n",
    "\n",
    "**OpenStreetMap Solar Panels and Arrays (OSM)**\n",
    "* Array and panel objects were downloaded _osmnx_ package in `script0_getOSMdata.ipynb`\n",
    "* Last OSM scrape: 12-11-2024\n",
    "* Previously, we used data from:\n",
    "    * Harmonzied Global Wind and Solar Farm Locations (HGLOBS)\n",
    "    * Downloaded from [figshare](https://doi.org/10.6084/m9.figshare.11310269.v6)\n",
    "    * Last Downloaded: 07-23-2024\n",
    "    * Version 6.0\n",
    "\n",
    "**TransitionZero Global Solar Asset Mapper (SAM)**\n",
    "* Downloaded from [TZ-SAM Portal](https://zenodo.org/records/11368204)\n",
    "* Last Download: 12-11-2024 \n",
    "* Other information: [Website](https://solar.transitionzero.org/), [Viewer](https://solar-map.transitionzero.org/), [SciData Preprint](https://zenodo.org/records/11368204/files/tz-sam_scientific_data.pdf?download=1)\n",
    "* Version Q3-2024 (Version 2)\n",
    "* Follow-on project containing all information from [Kruitwagen et al., 2021](https://zenodo.org/records/5005868). \n",
    "* NOTE: TZ-SAM also contains *raw_polygons*, which are all overlapping polygon shapefiles from all sources (including prior TZ-SAM versions). Could be useful in the future, or even a pathway that we use to share data. \n",
    "\n",
    "## Array Point-Level Data\n",
    "\n",
    "**NREL Innovative Solar Practices Integrated with Rural Economies and Ecosystems (InSPIRE) Database**\n",
    "* Downloaded from [InSPIRE Portal](https://openei.org/wiki/InSPIRE/Agrivoltaics_Map)\n",
    "* Last Download: 12-11-2024 \n",
    "\n",
    "**LBNL Utility-Scale Solar (USS), 2024 Edition**\n",
    "* Downloaded from [LBNL Utility-Scale Solar Portal](https://emp.lbl.gov/utility-scale-solar/)\n",
    "* Last Downloaded: 11-16-2024 (Up-to-date as of 12-11-2024)\n",
    "* Large excel report, project level data was copied from original report .xlsx to a new .csv from Individual_Project_Data tab\n",
    "\n",
    "**NREL PV Data Acquisition (PV-DAQ) Database**\n",
    "* Downloaded from [PV-DAQ Portal - Available Systems Information](https://data.openei.org/submissions/4568), and [PVDAQ Data Map](https://openei.org/wiki/PVDAQ/PVData_Map)\n",
    "* Last Downloaded: 07-23-2024 (Up-to-date as of 12-11-2024)\n",
    "\n",
    "**International Energy Agency (IEA) & NREL Solar Power and Chemical Energy System (SolarPACES) Database**\n",
    "* Downloaded from [Project Page](https://solarpaces.nrel.gov/)\n",
    "* Last Downloaded: 07-29-2024 (Up-to-date as of 12-11-2024)\n",
    "* More information at [US CSP Project Pages](https://solarpaces.nrel.gov/by-country/US)\n",
    "* While SolarPACES is the overarching project (and how we refer to the dataset here), the product is called [CSP.guru](https://csp.guru/)\n",
    "\n",
    "**Global Solar Power Tracker (GSPT) from Global Energy Monitor (GEM) and TransistionZero**\n",
    "* Downloaded from [GEM Portal](https://globalenergymonitor.org/download-data-success/)\n",
    "* Last Downloaded: 07-24-2024 (Up-to-date as of 12-11-2024)\n",
    "* Access request required\n",
    "\n",
    "**World Resource Institute (WRI) Global Power Plant Database (GPPDB)**\n",
    "* Downloaded from [WRI Portal](https://datasets.wri.org/dataset/globalpowerplantdatabase)\n",
    "* Last Downloaded: 07-30-2024 (Up-to-date as of 12-11-2024)\n",
    "* Version 1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os \n",
    "\n",
    "# Load config file\n",
    "def load_config(filename):\n",
    "    config = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            # Strip whitespace and split by '='\n",
    "            key, value = line.strip().split('=')\n",
    "            # Try to convert to numeric values if possible\n",
    "            try:\n",
    "                value = float(value) if '.' in value else int(value)\n",
    "            except ValueError:\n",
    "                pass  # Leave as string if not a number\n",
    "            config[key] = value\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot find header.dxf (GDAL_DATA is not defined)\n"
     ]
    }
   ],
   "source": [
    "# Set folder paths\n",
    "wd = r'S:\\Users\\stidjaco\\R_files\\BigPanel'\n",
    "downloaded_path = os.path.join(wd, r'Data\\Downloaded')\n",
    "derived_path = os.path.join(wd, r'Data\\Derived')\n",
    "derivedTemp_path = os.path.join(derived_path, r'intermediateProducts')\n",
    "\n",
    "# Set CONUS Shapefile path\n",
    "CONUS_path = os.path.join(wd, r'Data\\Downloaded\\CONUS_NoGreatLakes\\CONUS_No_Great_Lakes.shp')\n",
    "\n",
    "# Set file paths\n",
    "uspvdb_path = os.path.join(downloaded_path, r'SolarDB\\USPVDB\\uspvdb_v2_0_20240801.shp')\n",
    "ccvpv_ArraysPath = os.path.join(downloaded_path, r'SolarDB\\CCVPV\\PV_ID_CV.shp')\n",
    "ccvpv_PanelsPath = os.path.join(downloaded_path, r'SolarDB\\CCVPV\\PV_ID_panels.shp')\n",
    "cwsd_ArraysYearPath = os.path.join(downloaded_path, r'SolarDB\\CWSD\\CPK_solarJun22_firstyear.geojson')\n",
    "cwsd_ArraysAllYearPath = os.path.join(downloaded_path, r'SolarDB\\CWSD\\CPK_solarJun22_annual.geojson')\n",
    "cwsd_NYarraysPath = os.path.join(downloaded_path, r'SolarDB\\CWSD\\ny_footprints\\ny_footprints.shp')\n",
    "cwsd_DEarraysPath = os.path.join(downloaded_path, r'SolarDB\\CWSD\\de_footprints\\de_footprints.shp')\n",
    "cwsd_VAarraysPath = os.path.join(downloaded_path, r'SolarDB\\CWSD\\va_footprints\\va_footprints.shp')\n",
    "cwsd_MDarraysPath = os.path.join(downloaded_path, r'SolarDB\\CWSD\\md_footprints\\md_footprints.shp')\n",
    "cwsd_PAarraysPath = os.path.join(downloaded_path, r'SolarDB\\CWSD\\pa_footprints\\pa_footprints.shp')\n",
    "osm_ArraysPath = os.path.join(downloaded_path, r'SolarDB\\OSM\\OSMSolarArrays.shp')\n",
    "osm_PanelsPath = os.path.join(downloaded_path, r'SolarDB\\OSM\\OSMSolarPanels.shp')\n",
    "sam_path = os.path.join(downloaded_path, r'SolarDB\\TZSAM\\runs_2024-Q3_outputs_external_analysis_polygons.gpkg')\n",
    "inspire_path = os.path.join(downloaded_path, r'SolarDB\\InSPIRE\\result.csv')\n",
    "lbnlUss_path = os.path.join(downloaded_path, r'SolarDB\\LBNLUSS\\USS_2024_Individual_Project_Data.csv')\n",
    "pvdaq_SysPath = os.path.join(downloaded_path, r'SolarDB\\PVDAQ\\systems.csv')\n",
    "pvdaq_MapPath = os.path.join(downloaded_path, r'SolarDB\\PVDAQ\\result.csv')\n",
    "solarPaces_path = os.path.join(downloaded_path, r'SolarDB\\SolarPACES\\csp-guru.csv')\n",
    "gspt_path = os.path.join(downloaded_path, r'SolarDB\\GSPT\\Global-Solar-Power-Tracker-June-2024.xlsx')\n",
    "gppdb_path = os.path.join(downloaded_path, r'SolarDB\\GPPDB\\global_power_plant_database.csv')\n",
    "\n",
    "# Get US Boundary to subset global/non-CONUS datasets\n",
    "uspvdb = gpd.read_file(uspvdb_path) # USPVDB shapefile\n",
    "US_boundary = gpd.read_file(CONUS_path) # CONUS boundary shapefile\n",
    "US_boundary = US_boundary.set_crs(epsg=4269) # Native projection of US boundary - NAD83\n",
    "US_boundary = US_boundary.to_crs(uspvdb.crs) # Transform to projection of USPVDB\n",
    "US_boundary['geometry'] = US_boundary.buffer(10) # Buffer US boundary by 10 meters to ensure that array bounds are not clipped\n",
    "\n",
    "# Load the config from the text file\n",
    "config = load_config('config.txt')\n",
    "\n",
    "# Set variables\n",
    "mostRecentInstallYear = config['mostRecentInstallYear'] # Most recent installation year of the datasets that we consider due to remote sensing data availability (full year)\n",
    "acre_to_m2 = config['acre_to_m2'] # 1 acre = 4046.86 m2\n",
    "gee_crs = config['gee_crs'] # native projection of Google Earth Engine exports\n",
    "overlapDist = config['overlapDist'] # 190 meters, Set a overlap distance for checking if points/mismatched geometries between Solar PV datasets are duplicates\n",
    "panelArrayBuff = config['panelArrayBuff'] # 10 meters, Set a distance for checking if points/mismatched geometries between Solar PV datasets are part of the same array\n",
    "minPanelRowArea = config['minPanelRowArea'] # 15 m2, minimum area for a single panel row from the 1st percentile panel area from Stid et al., 2022. Filter small sub-panel chunks\n",
    "\n",
    "# Create a function to format the data to the schema\n",
    "def formatDf(df, nativeIdentifier, installationYear, capacityMWdc, area_m2, moduleType, agrivoltaicType, azimuth, mountTechnology, source):\n",
    "    # Change column names to match the schema\n",
    "    df = df.rename(columns={nativeIdentifier: 'nativeID', capacityMWdc: 'cap_mw', area_m2: 'area', installationYear: 'instYr', moduleType: 'modType', azimuth: 'azimuth', mountTechnology: 'mount', agrivoltaicType: 'AVtype'})\n",
    "\n",
    "    # Set source\n",
    "    df['Source'] = source\n",
    "\n",
    "    # Fill empy numeric column rows with -9999, and empty string column rows with NaN\n",
    "    df['cap_mw'] = df['cap_mw'].fillna(-9999)\n",
    "    df['area'] = df['area'].fillna(-9999)\n",
    "    df['instYr'] = df['instYr'].fillna(-9999)\n",
    "    df['azimuth'] = df['azimuth'].fillna(-9999)\n",
    "    df['modType'] = df['modType'].fillna('')\n",
    "    df['AVtype'] = df['AVtype'].fillna('')\n",
    "    df['mount'] = df['mount'].fillna('')\n",
    "\n",
    "    # Force data types to match schema\n",
    "    df['nativeID'] = df['nativeID'].astype(str)\n",
    "    df['instYr'] = df['instYr'].astype(int)\n",
    "    df['cap_mw'] = df['cap_mw'].astype(float)\n",
    "    df['area'] = df['area'].astype(float)\n",
    "    df['azimuth'] = df['azimuth'].astype(float)\n",
    "    df['modType'] = df['modType'].astype(str)\n",
    "    df['modType'] = df['modType'].str.lower() # Ensure modtype is lowercase\n",
    "    df['AVtype'] = df['AVtype'].astype(str)\n",
    "    df['AVtype'] = df['AVtype'].str.lower() # Ensure AVtype is lowercase\n",
    "    df['mount'] = df['mount'].astype(str)\n",
    "    df['mount'] = df['mount'].str.lower() # Ensure mount is\n",
    "\n",
    "    # As a default, if modType is not c-si or csp or thin-film, set to c-si. We use this information in the image classification.\n",
    "    df.loc[~df['modType'].isin(['c-si', 'csp', 'thin-film']), 'modType'] = 'c-si'\n",
    "\n",
    "    # Select schema columns\n",
    "    df = df[['nativeID', 'instYr', 'cap_mw', 'area', 'modType', 'AVtype', 'azimuth', 'mount', 'Source', 'geometry']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare United States Photovoltaic Database (USPVDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of arrays in USPVDB in the US is 4185\n",
      "Total area of arrays in USPVDB in the US is 1605.532313 km2\n"
     ]
    }
   ],
   "source": [
    "# Call data\n",
    "uspvdb = gpd.read_file(uspvdb_path)\n",
    "\n",
    "# Set a column for agrivoltaic type and fill with NaN\n",
    "uspvdb['AVtype'] = np.nan\n",
    "\n",
    "# Set a mount column. \n",
    "# If p_axis is 'single-axis', set to single_axis. If p_axis is 'fixed-axis', set to fixed_axis. If p_axis is 'dual-axis', set to dual_axis. If p_axis is 'fixed-tilt,single-axis' or 'fixed-tilt,single-axis,dual-axis', set to mixed\n",
    "uspvdb['mount'] = np.nan\n",
    "uspvdb['mount'] = uspvdb['mount'].astype(object)\n",
    "uspvdb.loc[uspvdb['p_axis'] == 'single-axis', 'mount'] = 'single_axis'\n",
    "uspvdb.loc[uspvdb['p_axis'] == 'fixed-tilt', 'mount'] = 'fixed_axis'\n",
    "uspvdb.loc[uspvdb['p_axis'] == 'dual-axis', 'mount'] = 'dual_axis'\n",
    "uspvdb.loc[uspvdb['p_axis'].isin(['fixed-tilt,single-axis', 'fixed-tilt,single-axis,dual-axis']), 'mount'] = 'mixed'\n",
    "\n",
    "# Format data\n",
    "uspvdb = formatDf(df = uspvdb, nativeIdentifier = 'case_id', installationYear = 'p_year', capacityMWdc = 'p_cap_dc', area_m2 = 'p_area', moduleType = 'p_tech_sec', agrivoltaicType = 'AVtype', azimuth = 'p_azimuth', mountTechnology = 'mount', source = 'USPVDB')\n",
    "\n",
    "# Print the number of unique arrays\n",
    "print(f'Number of arrays in USPVDB in the US is {len(uspvdb)}')\n",
    "\n",
    "# Print the total area in km2 of the arrays\n",
    "print(f'Total area of arrays in USPVDB in the US is {uspvdb[\"area\"].sum() / 1e6} km2')\n",
    "\n",
    "# Export to shapefile\n",
    "uspvdb.to_file(os.path.join(derivedTemp_path, r'uspvdb_poly.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare California's Central Valley Photoltaic Dataset (CCVPV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of arrays in CCVPV in the US is 1006\n",
      "The total area of arrays in CCVPV in the US is 58.90118407347751 km2\n"
     ]
    }
   ],
   "source": [
    "# Call data\n",
    "ccvpv = gpd.read_file(ccvpv_ArraysPath)\n",
    "\n",
    "# Transform to projection of uspvdb\n",
    "ccvpv = ccvpv.set_crs(epsg=4326) # Native projection dataset - WGS84\n",
    "ccvpv = ccvpv.to_crs(uspvdb.crs)\n",
    "\n",
    "# Set a column for agrivoltaic type and fill with NaN\n",
    "ccvpv['AVtype'] = np.nan\n",
    "\n",
    "# Set a column for moduleType and assume all modules are 'c-si'\n",
    "ccvpv['modType'] = 'c-si'\n",
    "\n",
    "# Set an empty column for azimuth filled with -9999\n",
    "ccvpv['azimuth'] = -9999\n",
    "\n",
    "# Set a mount column. If Class is 'Si_Fixed_S', set to fixed_axis. If Class is 'Si_Single_E/W', set to single_axis. \n",
    "ccvpv['mount'] = np.nan\n",
    "ccvpv['mount'] = ccvpv['mount'].astype(object)\n",
    "ccvpv.loc[ccvpv['Class'] == 'Si_Fixed_S', 'mount'] = 'fixed_axis'\n",
    "ccvpv.loc[ccvpv['Class'] == 'Si_Single_E/W', 'mount'] = 'single_axis'\n",
    "\n",
    "# Format data\n",
    "ccvpv = formatDf(df = ccvpv, nativeIdentifier = 'Index', installationYear = 'Yr_inst', capacityMWdc = 'TPVPp', area_m2 = 'Tot_a', moduleType = 'modType', agrivoltaicType = 'AVtype', azimuth = 'azimuth', mountTechnology = 'mount', source = 'CCVPV')\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Some corrections specific to CCVPV\n",
    "\n",
    "# We will omit the negative buffer from the CCVPV array preparation. \n",
    "# Although we are highly confident that this dataset contains near-zero comissions and omissions, and we are fully aware of the array generation methods (buffer 5m, dissolve), there is benefit to keeping the boundary.\n",
    "# The negative buffer provides a more accurate representation of the array boundary, particularly for smaller arrays, but maintaining more area (still bounded by actual panel-row and spacing) allows for edge pixels to be included in the image classification. \n",
    "# The unbuffering here can also lead to some erroneous array shape artifacts, that we deal with after getPanels.\n",
    "\n",
    "# Unbuffer ccvpv arrays by -5 meters (based on derivation methods from Stid et al., 2022)\n",
    "#ccvpv['geometry'] = ccvpv.buffer(-5)\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Export\n",
    "\n",
    "# Print the number of unique arrays\n",
    "print(f'The number of arrays in CCVPV in the US is {len(ccvpv)}')\n",
    "\n",
    "# Print the total area in km2 of the arrays\n",
    "print(f'The total area of arrays in CCVPV in the US is {ccvpv[\"area\"].sum() / 1e6} km2')\n",
    "\n",
    "# For now, export only array shapefile\n",
    "ccvpv.to_file(os.path.join(derivedTemp_path, r'ccvpv_poly.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Chesapeake Watershed Solar Data (CWSD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original CWSD arrays pre-overlap filtering: 1465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stidjaco\\AppData\\Local\\Temp\\ipykernel_45960\\1160690457.py:87: FutureWarning: Currently, index_parts defaults to True, but in the future, it will default to False to be consistent with Pandas. Use `index_parts=True` to keep the current behavior and True/False to silence the warning.\n",
      "  cwsdDissolved = cwsd.dissolve(aggfunc='max').explode().reset_index(drop=True)\n",
      "C:\\Users\\stidjaco\\AppData\\Local\\Temp\\ipykernel_45960\\1160690457.py:110: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  cwsdDissolved.to_file(os.path.join(derivedTemp_path, r'cwsd_poly.shp'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of arrays in CWSD in the US is 1352\n",
      "The total area of arrays in CWSD in the US is 59.33720047380677 km2\n"
     ]
    }
   ],
   "source": [
    "# Call in CWSD first year and all year data\n",
    "cwsd_firstYear = gpd.read_file(cwsd_ArraysYearPath)\n",
    "cwsd_allYear = gpd.read_file(cwsd_ArraysAllYearPath)\n",
    "\n",
    "# Call in CWSD state training data\n",
    "cwsd_NY = gpd.read_file(cwsd_NYarraysPath)\n",
    "cwsd_DE = gpd.read_file(cwsd_DEarraysPath)\n",
    "cwsd_VA = gpd.read_file(cwsd_VAarraysPath)\n",
    "cwsd_MD = gpd.read_file(cwsd_MDarraysPath)\n",
    "cwsd_PA = gpd.read_file(cwsd_PAarraysPath)\n",
    "\n",
    "# Merge all state training data\n",
    "cwsdTraining = pd.concat([cwsd_NY, cwsd_DE, cwsd_VA, cwsd_MD, cwsd_PA])\n",
    "\n",
    "# Set first year as desired cwsd gdf\n",
    "cwsd = cwsd_firstYear.copy()\n",
    "\n",
    "# Set native crs (EPSG:4326) to projection of USPVDB\n",
    "cwsd = cwsd.set_crs(epsg=4326) # Native projection dataset - WGS84\n",
    "cwsd = cwsd.to_crs(uspvdb.crs)\n",
    "#cwsdTraining = cwsdTraining.set_crs(epsg=4269) # Native projection dataset - GCS North American 1983\n",
    "cwsdTraining = cwsdTraining.to_crs(uspvdb.crs)\n",
    "\n",
    "# Add year_right column from cwsd to cwsdTraining\n",
    "cwsdTraining = gpd.sjoin(cwsdTraining, cwsd[['geometry', 'year_right']], how='left', predicate='intersects')\n",
    "\n",
    "# Drop all columns except geometry and year_right for both cwsd and cwsdTraining\n",
    "cwsd = cwsd[['geometry', 'year_right']]\n",
    "cwsdTraining = cwsdTraining[['geometry', 'year_right']]\n",
    "\n",
    "# Drop cwsd that intersects with cwsdTraining\n",
    "cwsd = cwsd[~cwsd.intersects(cwsdTraining.unary_union)]\n",
    "\n",
    "# Merge cwsd and cwsdTraining\n",
    "cwsd = pd.concat([cwsd, cwsdTraining])\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Some corrections specific to CWSD\n",
    "\n",
    "# CWSD contains self intersections due to Evans et al., 2023 methodology. We will remove these self intersections by dissolving the arrays.\n",
    "cwsdAttributes = cwsd.copy()\n",
    "cwsd['temp'] = 1\n",
    "cwsd = cwsd.dissolve(by='temp')\n",
    "cwsd = cwsd.reset_index(drop=True)\n",
    "\n",
    "# Drop all columns except geometry\n",
    "cwsd = cwsd[['geometry']]\n",
    "\n",
    "# Explode the dissolved arrays and spatially join with the original attributes\n",
    "cwsd = cwsd.explode(index_parts=False)\n",
    "cwsd = cwsd.reset_index(drop=True)\n",
    "cwsd = gpd.sjoin(cwsd, cwsdAttributes, how='left', predicate='intersects')\n",
    "\n",
    "# Set a column for agrivoltaic type and fill with NaN\n",
    "cwsd['AVtype'] = np.nan\n",
    "\n",
    "# Set a column for moduleType and assume all modules are 'c-si'\n",
    "cwsd['modType'] = 'c-si'\n",
    "\n",
    "# Set an empty column for azimuth filled with -9999\n",
    "cwsd['azimuth'] = -9999\n",
    "\n",
    "# Set a column for mount and fill with NaN\n",
    "cwsd['mount'] = np.nan\n",
    "\n",
    "# Set a column for cap_mw and fill with NaN\n",
    "cwsd['cap_mw'] = np.nan\n",
    "\n",
    "# If it has not been already, drop initial area column, then set a coloumn for area and set polygon area to this column\n",
    "cwsd = cwsd.drop(columns=['area'], errors='ignore')\n",
    "cwsd['area'] = cwsd['geometry'].area\n",
    "\n",
    "# Set a nativeID which is 1 to the length of the dataframe\n",
    "cwsd['nativeID'] = range(1, len(cwsd) + 1)\n",
    "\n",
    "# CWSD year_right is the installation year, but is limited by Sentinel-2 data availability. Therefore, we will set any year_right equal to 2017 (min S2 year) as -9999, and retain the remaining years.\n",
    "cwsd.loc[cwsd['year_right'] <= 2017, 'year_right'] = -9999\n",
    "\n",
    "# Format data\n",
    "cwsd = formatDf(df = cwsd, nativeIdentifier = 'nativeID', installationYear = 'year_right', capacityMWdc = 'cap_mw', area_m2 = 'area', moduleType = 'modType', agrivoltaicType = 'AVtype', azimuth = 'azimuth', mountTechnology = 'mount', source = 'CWSD')\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ CWSD contains repeat (overlapping) arrays. Remove duplicates\n",
    "\n",
    "# Print the number of unique arrays\n",
    "print(f'Original CWSD arrays pre-overlap filtering: {len(cwsd)}')\n",
    "\n",
    "# If any cwsd arrays overlap, remove duplicates by keeping the largest array.\n",
    "cwsdDissolved = cwsd.dissolve(aggfunc='max').reset_index(drop=True).explode().reset_index(drop=True)\n",
    "\n",
    "# Recalculate area\n",
    "cwsdDissolved['area'] = cwsdDissolved['geometry'].area\n",
    "\n",
    "# Set a tempID\n",
    "cwsdDissolved['tempID'] = range(1, len(cwsdDissolved) + 1)\n",
    "\n",
    "# Drop nativeID column, spatially join with original cwsd to get nativeID, drop duplicates by tempID\n",
    "cwsdDissolved = cwsdDissolved.drop(columns=['nativeID'], errors='ignore')\n",
    "cwsdDissolved = gpd.sjoin(cwsdDissolved, cwsd[['nativeID', 'geometry']], how='left', predicate='intersects')\n",
    "cwsdDissolved = cwsdDissolved.drop_duplicates(subset='tempID')\n",
    "cwsdDissolved = cwsdDissolved.drop(columns=['tempID'], errors='ignore')\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Export\n",
    "\n",
    "# Print the number of unique arrays\n",
    "print(f'The number of arrays in CWSD in the US is {len(cwsdDissolved)}')\n",
    "\n",
    "# Print the total area in km2 of the arrays\n",
    "print(f'The total area of arrays in CWSD in the US is {cwsdDissolved[\"area\"].sum() / 1e6} km2')\n",
    "\n",
    "# Export to shapefile\n",
    "cwsdDissolved.to_file(os.path.join(derivedTemp_path, r'cwsd_poly.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare OpenStreetMap Dataset (OSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of arrays in OSM in the US is 10531\n",
      "The total area of arrays in OSM in the US is 2438.020597214556 km2\n"
     ]
    }
   ],
   "source": [
    "# Call data\n",
    "osm = gpd.read_file(osm_ArraysPath)\n",
    "\n",
    "# OSM data is already in USPVDB projection and exclusive to CONUS.\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Some corrections specific to OSM\n",
    "\n",
    "# Print unique Source values and counts\n",
    "#print(osm['Source'].value_counts())\n",
    "\n",
    "# Set a column for agrivoltaic type and fill with NaN\n",
    "osm['AVtype'] = np.nan\n",
    "\n",
    "# Set an empty column for azimuth filled with -9999\n",
    "osm['azimuth'] = -9999\n",
    "\n",
    "# Set an empty column for mount filled with NaN\n",
    "osm['mount'] = np.nan\n",
    "osm['mount'] = osm['mount'].astype(object)\n",
    "\n",
    "# Format data\n",
    "osm = formatDf(df = osm, nativeIdentifier = 'nativeID', installationYear = 'instYr', capacityMWdc = 'cap_mw', area_m2 = 'area', moduleType = 'modType', agrivoltaicType = 'AVtype', azimuth = 'azimuth', mountTechnology = 'mount', source = 'OSM')\n",
    "\n",
    "# OSM nativeID is not unique. We took nativeID from osmid, but there was an issue with the data. The OSM download will retain the osmid nativeID, we will save a new one here. \n",
    "osm = osm.reset_index()\n",
    "osm['nativeID'] = osm.index.astype(str)\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Export\n",
    "\n",
    "# Print the number of unique arrays\n",
    "print(f'The number of arrays in OSM in the US is {len(osm)}')\n",
    "\n",
    "# Print the total area in km2 of the arrays\n",
    "print(f'The total area of arrays in OSM in the US is {osm[\"area\"].sum() / 1e6} km2')\n",
    "\n",
    "# Export shapefile\n",
    "osm.to_file(os.path.join(derivedTemp_path, r'osm_poly.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare TransitionZero Solar Asset Mapper (SAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of arrays in SAM in the US is 12208\n",
      "The total area of arrays in SAM in the US is 3910.159133517089 km2\n"
     ]
    }
   ],
   "source": [
    "# Call data\n",
    "sam = gpd.read_file(sam_path)\n",
    "\n",
    "# Transform to projection of USPVDB\n",
    "sam = sam.to_crs(uspvdb.crs)\n",
    "\n",
    "# For now, subset sam to only include the area of the US\n",
    "sam = sam[sam.intersects(US_boundary.unary_union)]\n",
    "\n",
    "# Set a column for agrivoltaic type and fill with NaN\n",
    "sam['AVtype'] = np.nan\n",
    "\n",
    "# Calculate area and add as a new column\n",
    "sam['area'] = sam['geometry'].area\n",
    "\n",
    "# Set a column for moduleType and assume all modules are 'c-si'\n",
    "sam['modType'] = 'c-si'\n",
    "\n",
    "# Set an empty column for azimuth filled with -9999\n",
    "sam['azimuth'] = -9999\n",
    "\n",
    "# Set an empty column for mount filled with NaN\n",
    "sam['mount'] = np.nan\n",
    "sam['mount'] = sam['mount'].astype(object)\n",
    "\n",
    "# Format constructed_before and constructed_after columns as dates. Current format is \"2017-12-21T15:41:18.469999+00:00\" or \"None\"\n",
    "sam['constructed_before'] = pd.to_datetime(sam['constructed_before'], errors='coerce')\n",
    "sam['constructed_after'] = pd.to_datetime(sam['constructed_after'], errors='coerce')\n",
    "\n",
    "# Get the median date between constructed_before and constructed_after\n",
    "sam['instYr'] = sam[['constructed_before', 'constructed_after']].mean(axis=1).dt.year\n",
    "\n",
    "# If instYr is NaN or equal to or less than 2017, or greater than or equal to 2024, set to -9999 -- we now allow for 2024 as the most recent year\n",
    "sam.loc[sam['instYr'].isnull(), 'instYr'] = -9999\n",
    "sam.loc[sam['instYr'] <= 2017, 'instYr'] = -9999\n",
    "#sam.loc[sam['instYr'] >= 2024, 'instYr'] = -9999\n",
    "\n",
    "# Format data\n",
    "sam = formatDf(df = sam, nativeIdentifier = 'cluster_id', installationYear = 'instYr', capacityMWdc = 'capacity_mw', area_m2 = 'area', moduleType = 'modType', agrivoltaicType = 'AVtype', azimuth = 'azimuth', mountTechnology = 'mount', source = 'SAM')\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Some corrections specific to SAM \n",
    "\n",
    "# Don't unbuffer here, as we need to keep the original geometry for the following random forest model\n",
    "\n",
    "# # Some cluster_id values are repeated, so we need to drop duplicates\n",
    "sam = sam.drop_duplicates(subset='nativeID')\n",
    "\n",
    "# Explode multipolygons. Many are erroneous, or contain rooftop arrays next to ground mounted. We need to append the 'nativeID' columns with '_n' where n is the index of the polygon in the multipolygon\n",
    "sam = sam.explode(index_parts=False) # We'll do this manually\n",
    "sam['nativeID'] = sam['nativeID'] + '_' + sam.groupby('nativeID').cumcount().astype(str)\n",
    "\n",
    "# Drop rows where the geometry type is None (some can arrise from the explode function)\n",
    "sam = sam[~sam[\"geometry\"].isnull()]\n",
    "\n",
    "# Drop area column, and recalculate area\n",
    "sam = sam.drop(columns=['area'], errors='ignore')\n",
    "sam['area'] = sam['geometry'].area\n",
    "\n",
    "# Reset index\n",
    "sam = sam.reset_index(drop=True)\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Export\n",
    "\n",
    "# Print the number of unique arrays\n",
    "print(f'The number of arrays in SAM in the US is {len(sam)}')\n",
    "\n",
    "# Print the total area in km2 of the arrays\n",
    "print(f'The total area of arrays in SAM in the US is {sam[\"area\"].sum() / 1e6} km2')\n",
    "\n",
    "# Export shapefile\n",
    "sam.to_file(os.path.join(derivedTemp_path, r'sam_poly.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Existing Solar PV Array Shapefile with Ordered Importance\n",
    "NOTE: If this code chunk changes (e.g., reordering preference or a new dataset is included), `script7` **Return GMSEUSgeorect Source Attribute to Original Spatial Source** must also be updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique arrays in the merged dataset is 16661\n",
      "The total area of arrays in the merged dataset is 3051.2023025288795 km2\n"
     ]
    }
   ],
   "source": [
    "# Call polygon data\n",
    "uspvdb = gpd.read_file(os.path.join(derivedTemp_path, r'uspvdb_poly.shp'))\n",
    "ccvpv = gpd.read_file(os.path.join(derivedTemp_path, r'ccvpv_poly.shp'))\n",
    "cwsd = gpd.read_file(os.path.join(derivedTemp_path, r'cwsd_poly.shp'))\n",
    "osm = gpd.read_file(os.path.join(derivedTemp_path, r'osm_poly.shp'))\n",
    "sam = gpd.read_file(os.path.join(derivedTemp_path, r'sam_poly.shp'))\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Merge datasets removing duplicates\n",
    " \n",
    "# Some datasets have multipolygons that add value, and are grouped togehter either erroneously or with updated data compared to higher-spatial qualtiy datasets. \n",
    "# To navigate this, we'll exlode the datasets, remove sub-shapes by intersection and spatial-quality, then dissolve by nativeID to return best available dataset.\n",
    "\n",
    "# First, give each dataset a unique identifier that does not repeat across datasets. We'll use the 'tempID' column for this, which is the row index of the dataset + Source as a string. We will reset this index later.\n",
    "uspvdb['tempID'] = uspvdb.index.astype(str) + '_USPVDB'\n",
    "ccvpv['tempID'] = ccvpv.index.astype(str) + '_CCVPV'\n",
    "cwsd['tempID'] = cwsd.index.astype(str) + '_CWSD'\n",
    "osm['tempID'] = osm.index.astype(str) + '_OSM'\n",
    "sam['tempID'] = sam.index.astype(str) + '_SAM'\n",
    "\n",
    "# Explode each dataset to polygons, except USPVDB (maintain quality of USPVDB)\n",
    "ccvpv = ccvpv.explode(index_parts=False)\n",
    "cwsd = cwsd.explode(index_parts=False)\n",
    "osm = osm.explode(index_parts=False)\n",
    "sam = sam.explode(index_parts=False)\n",
    "\n",
    "# Remove arrays with overlap in the following level of priority: USPVDB, CCVPV, HGLOBS, SAM\n",
    "ccvpv = ccvpv[~ccvpv.intersects(uspvdb.unary_union)]\n",
    "cwsd = cwsd[~cwsd.intersects(uspvdb.unary_union)]\n",
    "cwsd = cwsd[~cwsd.intersects(ccvpv.unary_union)]\n",
    "osm = osm[~osm.intersects(uspvdb.unary_union)]\n",
    "osm = osm[~osm.intersects(ccvpv.unary_union)]\n",
    "osm = osm[~osm.intersects(cwsd.unary_union)]\n",
    "sam = sam[~sam.intersects(uspvdb.unary_union)]\n",
    "sam = sam[~sam.intersects(ccvpv.unary_union)]\n",
    "sam = sam[~sam.intersects(cwsd.unary_union)]\n",
    "sam = sam[~sam.intersects(osm.unary_union)] \n",
    "\n",
    "# Dissolve by tempID, maintain both columns. \n",
    "ccvpv = ccvpv.dissolve(by=['tempID'], as_index=False)\n",
    "cwsd = cwsd.dissolve(by=['tempID'], as_index=False)\n",
    "osm = osm.dissolve(by=['tempID'], as_index=False)\n",
    "sam = sam.dissolve(by=['tempID'], as_index=False)\n",
    "\n",
    "# Merge all datasets\n",
    "merged = gpd.GeoDataFrame(pd.concat([uspvdb, ccvpv, cwsd, osm, sam], ignore_index=True), crs=uspvdb.crs)\n",
    "\n",
    "# Mask the merged dataset with the US boundary (this is a redundant step, is a good check)\n",
    "merged = merged[merged.intersects(US_boundary.unary_union)]\n",
    "\n",
    "# Given the modifications to the datasets, we need to re-calculate the area. Also note that cap_mw is not longer accurate given the possibility of multipolygon alteration.\n",
    "merged['area'] = merged['geometry'].area\n",
    "\n",
    "# Depending on the source method, there may be some erroneous geometries created that have an area of 0 or near zero (e.g. unbuffer of CCVPV arrays). \n",
    "# The smallest real array/panel-row is ~28 m2. Remove all arrays smaller than this. \n",
    "merged = merged[merged['area'] >= minPanelRowArea]\n",
    "\n",
    "# Drop all columns that are not needed for the analysis\n",
    "merged = merged[['nativeID', 'instYr', 'cap_mw', 'area', 'modType', 'AVtype', 'azimuth', 'mount', 'Source', 'geometry']]\n",
    "\n",
    "# Print the number of unique arrays\n",
    "print(f'The number of unique arrays in the merged dataset is {len(merged)}')\n",
    "\n",
    "# Print the total area in km2 of the arrays\n",
    "print(f'The total area of arrays in the merged dataset is {merged[\"area\"].sum() / 1e6} km2')\n",
    "\n",
    "# Set a tempID that is the row index for the merged dataset\n",
    "merged = merged.reset_index(drop=True)\n",
    "merged['tempID'] = merged.index\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Check geometries and export\n",
    "\n",
    "# Due to the buffering and unbuffering of CCVPV and OSM arrays, and for unknown reasons in other datasets, some mulitpolygons contain erroneous geometries that result in a near-zero area, linestrings, or points.\n",
    "# To check for and remove these, we'll explode merged, calculate a temporary area, remove subarrays that are less than 28 m2, then dissolve by tempID.\n",
    "merged = merged.explode(index_parts=False)\n",
    "merged['tempArea'] = merged['geometry'].area\n",
    "merged = merged[merged['tempArea'] >= minPanelRowArea]\n",
    "merged = merged.dissolve(by=['tempID'], as_index=False)\n",
    "merged = merged.drop(columns=['tempArea'])\n",
    "merged = merged.reset_index(drop=True)\n",
    "\n",
    "# Export shapefile\n",
    "merged.to_file(os.path.join(derivedTemp_path, r'existingDatasetArrayShapes.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Solar Array Datasets with centroid spatial infomration, overlay with existing, and manually digitize missing array shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare NREL InSPIRE Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of arrays in InSPIRE in the US is 571\n"
     ]
    }
   ],
   "source": [
    "# Call data\n",
    "inspire = pd.read_csv(inspire_path)\n",
    "\n",
    "# Remove rows with NaN in the \"Coordinates\" column\n",
    "inspire = inspire.dropna(subset=['Coordinates'])\n",
    "\n",
    "# Split the \"Coordinates\" column into two columns: \"Latitude\" and \"Longitude\" (base column is \"41.843493, -90.036077\")\n",
    "inspire[['Latitude', 'Longitude']] = inspire['Coordinates'].str.split(', ', expand=True)\n",
    "\n",
    "# Create a centroid shape file from the Latitude and Longitude columns\n",
    "inspire['geometry'] = gpd.points_from_xy(inspire['Longitude'], inspire['Latitude'])\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "inspire = gpd.GeoDataFrame(inspire, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "# Set (WGS84) and Transform to projection of USPVDB\n",
    "inspire = inspire.set_crs(epsg=4326)\n",
    "inspire = inspire.to_crs(uspvdb.crs)\n",
    "\n",
    "# Mask inspire database to CONUS Boundary \n",
    "inspire = inspire[inspire.intersects(US_boundary.unary_union)]\n",
    "\n",
    "# Remove \"InSPIRE/Sites\" from all rows in the \"Name\" column\n",
    "inspire['Name'] = inspire['Name'].str.replace('InSPIRE/Sites/', '')\n",
    "\n",
    "# Site Size is in acres, convert to square meters\n",
    "inspire['Site Size'] = inspire['Site Size'] * acre_to_m2\n",
    "\n",
    "# Set a column for moduleType and assume all modules are 'c-si'. This dataset does contain 'PV Technology' column differentiating between Monofacial, Bifacial, and Translucent modules, but we will assume all are c-si for now.\n",
    "inspire['modType'] = 'c-si'\n",
    "\n",
    "# Set an empty column for azimuth filled with -9999\n",
    "inspire['azimuth'] = -9999\n",
    "\n",
    "# Create a mount column. If 'Type Of Array' is Single-axis Tracking, set to single_axis. If 'Type Of Array' is Fixed, set to fixed_axis.\n",
    "inspire['mount'] = np.nan\n",
    "inspire['mount'] = inspire['mount'].astype(object)\n",
    "inspire.loc[inspire['Type Of Array'] == 'Single-axis Tracking', 'mount'] = 'single_axis'\n",
    "inspire.loc[inspire['Type Of Array'] == 'Fixed', 'mount'] = 'fixed_axis'\n",
    "\n",
    "# Format data\n",
    "inspire = formatDf(df = inspire, nativeIdentifier = 'Name', installationYear = 'Year Installed', capacityMWdc = 'System Size', area_m2 = 'Site Size', moduleType = 'modType', agrivoltaicType = 'Habitat Type', azimuth = 'azimuth', mountTechnology = 'mount', source = 'InSPIRE')\n",
    "\n",
    "# Print the number of arrays\n",
    "print(f'The number of arrays in InSPIRE in the US is {len(inspire)}')\n",
    "\n",
    "# Export shapefile\n",
    "inspire.to_file(os.path.join(derivedTemp_path, r'inspire_point.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare LBNL-USS Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of arrays in LBNLUSE in the US is 1503\n"
     ]
    }
   ],
   "source": [
    "# Call LBNL USE data\n",
    "lbnlUss = pd.read_csv(lbnlUss_path)\n",
    "\n",
    "# Create a centroid shape file from the Latitude and Longitude columns\n",
    "lbnlUss['geometry'] = gpd.points_from_xy(lbnlUss['Longitude'], lbnlUss['Latitude'])\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "lbnlUss = gpd.GeoDataFrame(lbnlUss, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "# Set (WGS84) and Transform to projection of USPVDB\n",
    "lbnlUss = lbnlUss.set_crs(epsg=4326)\n",
    "lbnlUss = lbnlUss.to_crs(uspvdb.crs)\n",
    "\n",
    "# Mask lbnlUss database to CONUS Boundary \n",
    "lbnlUss = lbnlUss[lbnlUss.intersects(US_boundary.unary_union)]\n",
    "\n",
    "# Set a new column called modType. Set to lower case. \n",
    "# If Solar Tech Sub is c-si, cpv, combo (c-si, cpv), combo (c-si, thin-film, cpv), set to c-si. If Solar Tech Sub is thin-film, set to thin-film. If Solar Tech Sub is trough or tower, set to csp. Fill all other values with c-si\n",
    "lbnlUss['modType'] = lbnlUss['Solar Tech Sub'].str.lower()\n",
    "lbnlUss.loc[~lbnlUss['modType'].isin(['c-si', 'cpv', 'combo (c-si, cpv)', 'combo (c-si, thin-film, cpv)']), 'modType'] = 'c-si'\n",
    "lbnlUss.loc[lbnlUss['modType'] == 'thin-film', 'modType'] = 'thin-film'\n",
    "lbnlUss.loc[lbnlUss['modType'].isin(['trough', 'tower']), 'modType'] = 'csp'\n",
    "\n",
    "# Set an agrivoltaic type column and fill with NaN\n",
    "lbnlUss['AVtype'] = np.nan\n",
    "\n",
    "# Set a column for area in m2 and fill with -9999\n",
    "lbnlUss['area'] = -9999\n",
    "\n",
    "# Try to convert Azimuth column string to float. If fails, fill with NaN\n",
    "lbnlUss['Azimuth'] = pd.to_numeric(lbnlUss['Azimuth'], errors='coerce')\n",
    "\n",
    "# Set a mount column. If 'Tracking Type' is 'Single Axis', set to single_axis. If 'Tracking Type' is 'Fixed Tilt', set to fixed_axis. If 'Tracking Type' is 'Dual-Axis', set to dual_axis. If 'Tracking Type' is 'Fixed, Single, Double' or 'Fixed, Single', set to mixed. \n",
    "lbnlUss['mount'] = np.nan\n",
    "lbnlUss['mount'] = lbnlUss['mount'].astype(object)\n",
    "lbnlUss.loc[lbnlUss['Tracking Type'] == 'Single Axis', 'mount'] = 'single_axis'\n",
    "lbnlUss.loc[lbnlUss['Tracking Type'] == 'Fixed Tilt', 'mount'] = 'fixed_axis'\n",
    "lbnlUss.loc[lbnlUss['Tracking Type'] == 'Dual-Axis', 'mount'] = 'dual_axis'\n",
    "lbnlUss.loc[lbnlUss['Tracking Type'].isin(['Fixed, Single, Double', 'Fixed, Single']), 'mount'] = 'mixed'\n",
    "\n",
    "# Format data\n",
    "lbnlUss = formatDf(df = lbnlUss, nativeIdentifier = 'Project Name', installationYear = 'Solar COD Year', capacityMWdc = 'Solar Capacity MW-DC', area_m2 = 'area', moduleType = 'modType', agrivoltaicType = 'AVtype', azimuth = 'Azimuth', mountTechnology = 'mount', source = 'LBNLUSS')\n",
    "\n",
    "# Print the number of arrays\n",
    "print(f'The number of arrays in LBNLUSE in the US is {len(lbnlUss)}')\n",
    "\n",
    "# Export shapefile\n",
    "lbnlUss.to_file(os.path.join(derivedTemp_path, r'lbnlUss_point.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare NREL PV-DAQ Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of arrays in PVDAQ in the US is 16\n"
     ]
    }
   ],
   "source": [
    "# Call data\n",
    "pvdaq_sys = pd.read_csv(pvdaq_SysPath)\n",
    "pvdaq_map = pd.read_csv(pvdaq_MapPath)\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ # Prepare site and system names for merging\n",
    "\n",
    "# Remove \"PVDAQ/Sites/\" from system_public_name in pvdaq_map\n",
    "pvdaq_map['system_public_name'] = pvdaq_map['system_public_name'].str.replace('PVDAQ/Sites/', '')\n",
    "\n",
    "# Remove all system ids (in the form of \"[####] \", for 1 to 4 numbers) for system_public_name in pvdaq_sys\n",
    "pvdaq_sys['system_public_name'] = pvdaq_sys['system_public_name'].str.replace(r'\\[\\d{1,4}\\]\\s', '', regex=True)\n",
    "\n",
    "# Replace \"_\" with \" \" in system_public_name in pvdaq_sys\n",
    "pvdaq_sys['system_public_name'] = pvdaq_sys['system_public_name'].str.replace('_', ' ')\n",
    "\n",
    "# Try to merge on system_public_name\n",
    "pvdaq = pd.merge(pvdaq_map, pvdaq_sys, on='system_public_name', how='inner')\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ # Process and prepare PVDAQ data\n",
    "\n",
    "# Remove all rows with PVDAQArray Configuaration = \"Fixed Roof\"\n",
    "pvdaq = pvdaq[pvdaq['PVDAQArray Configuration'] != 'Fixed Roof']\n",
    "\n",
    "# Create a centroid shape file from the Latitude and Longitude columns\n",
    "pvdaq['geometry'] = gpd.points_from_xy(pvdaq['site_longitude'], pvdaq['site_latitude'])\n",
    "\n",
    "# Convert to GeoDataFrame and transform to projection of USPVDB\n",
    "pvdaq = gpd.GeoDataFrame(pvdaq, geometry='geometry', crs=4326)\n",
    "pvdaq = pvdaq.to_crs(uspvdb.crs)\n",
    "\n",
    "# Mask the US boundary\n",
    "pvdaq = pvdaq[pvdaq.intersects(US_boundary.unary_union)]\n",
    "\n",
    "# To start, assume that 'PVDAQfirst timestamp' is the installation year. In the format '12/29/2010  2:30:00 PM', extract the year. For arrays with timestampst that dont fit this format, set year to -9999\n",
    "pvdaq['system_year'] = pd.to_datetime(pvdaq['PVDAQfirst timestamp'], errors='coerce').dt.year.fillna(-9999)\n",
    "\n",
    "# SThe column 'PVDAQSystemSize' in in kWdc, convert to MWdc\n",
    "pvdaq['system_size'] = pvdaq['PVDAQSystemSize'] / 1000\n",
    "\n",
    "# Set a column for agrivoltaic type and fill with NaN\n",
    "pvdaq['AVtype'] = np.nan\n",
    "\n",
    "# Set a column for installation year and fill with NaN integer value\n",
    "pvdaq['Year'] = -9999\n",
    "\n",
    "# Set a column for area and fill with NaN\n",
    "pvdaq['area'] = np.nan\n",
    "\n",
    "# Set a column for moduleType and assume all modules are 'c-si'\n",
    "pvdaq['modType'] = 'c-si'\n",
    "\n",
    "# Set an empty column for azimuth filled with -9999\n",
    "pvdaq['azimuth'] = -9999\n",
    "\n",
    "# Set an empty column for mount filled with NaN\n",
    "pvdaq['mount'] = np.nan\n",
    "pvdaq['mount'] = pvdaq['mount'].astype(object)\n",
    "\n",
    "# Format data (used to use system_id, but public name gives more context for searches)\n",
    "pvdaq = formatDf(df = pvdaq, nativeIdentifier = 'system_public_name', installationYear = 'system_year', capacityMWdc = 'system_size', area_m2 = 'area', moduleType = 'modType', agrivoltaicType = 'AVtype', azimuth = 'azimuth', mountTechnology = 'mount', source = 'PVDAQ')\n",
    "\n",
    "# Print the number of arrays\n",
    "print(f'The number of arrays in PVDAQ in the US is {len(pvdaq)}')\n",
    "\n",
    "# Export \n",
    "pvdaq.to_file(os.path.join(derivedTemp_path, r'pvdaq_point.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare NREL & IEA SolarPaces Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of arrays in SolarPACES in the US is 13\n"
     ]
    }
   ],
   "source": [
    "# Call data\n",
    "solarPaces = pd.read_csv(solarPaces_path)\n",
    "\n",
    "# Subset for County = 'United States' and Status = 'Operational' or 'Currently Non-Operational'\n",
    "solarPaces = solarPaces[solarPaces['Country'] == 'United States']\n",
    "solarPaces = solarPaces[solarPaces['Status'].isin(['Operational', 'Currently Non-Operational'])]\n",
    "\n",
    "# Split Location_coordinates column into Latitude and Longitude (comma separated)\n",
    "solarPaces[['Latitude', 'Longitude']] = solarPaces['Location_coordinates'].str.split(',', expand=True)\n",
    "\n",
    "# Create a centroid shape file from the Latitude and Longitude columns\n",
    "solarPaces['geometry'] = gpd.points_from_xy(solarPaces['Longitude'].astype(float), solarPaces['Latitude'].astype(float))\n",
    "\n",
    "# Convert to GeoDataFrame and transform to projection of USPVDB\n",
    "solarPaces = gpd.GeoDataFrame(solarPaces, geometry='geometry', crs=4326)\n",
    "\n",
    "# Transform to projection of USPVDB\n",
    "solarPaces = solarPaces.to_crs(uspvdb.crs)\n",
    "\n",
    "# Mask for US boundary\n",
    "solarPaces = solarPaces[solarPaces.intersects(US_boundary.unary_union)]\n",
    "\n",
    "# Set a column for agrivoltaic type and fill with NaN\n",
    "solarPaces['AVType'] = np.nan\n",
    "\n",
    "# Set a column for area and fill with -9999\n",
    "solarPaces['area'] = -9999 # Land_area_whole_station_not_solar_field_km2 column exists, but is not solar field area\n",
    "\n",
    "# Set a modType column to 'csp' for all rows\n",
    "solarPaces['modType'] = 'csp'\n",
    "\n",
    "# Set an empty column for azimuth filled with -9999\n",
    "solarPaces['azimuth'] = -9999\n",
    "\n",
    "# Set a mount column. If Technology is 'Parabolic Trough', 'Linear Fresnel', 'Hybrid, Parabolic Trough', or Hybrid, Linear Fresnel', set to single_axis. If Technology is 'Power Tower', 'Dish', or Beam-Down Tower', set to dual_axis. We wont make assumptions about other hybrids. \n",
    "solarPaces['mount'] = np.nan\n",
    "solarPaces['mount'] = solarPaces['mount'].astype(object)\n",
    "solarPaces.loc[solarPaces['Technology'].isin(['Parabolic Trough', 'Linear Fresnel', 'Hybrid, Parabolic Trough', 'Hybrid, Linear Fresnel']), 'mount'] = 'single_axis'\n",
    "solarPaces.loc[solarPaces['Technology'].isin(['Power Tower', 'Dish', 'Beam-Down Tower']), 'mount'] = 'dual_axis'\n",
    "\n",
    "# Print the number of single_axis and dual_axis arrays\n",
    "print(f'The number of single_axis arrays in SolarPACES in the US is {len(solarPaces[solarPaces[\"mount\"] == \"single_axis\"])}')\n",
    "\n",
    "# Format data (used to be OpenCSP_ID, but Power_station gives more context for searches)\n",
    "solarPaces = formatDf(df = solarPaces, nativeIdentifier = 'Power_station', installationYear = 'Year_operational', capacityMWdc = 'Capacity_MW', area_m2 = 'area', moduleType = 'modType', agrivoltaicType = 'AVType', azimuth = 'azimuth', mountTechnology = 'mount', source = 'SolarPACES')\n",
    "\n",
    "# Print the number of arrays\n",
    "print(f'The number of arrays in SolarPACES in the US is {len(solarPaces)}')\n",
    "\n",
    "# Export \n",
    "solarPaces.to_file(os.path.join(derivedTemp_path, r'solarPaces_point.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of single_axis arrays in SolarPACES in the US is 8\n"
     ]
    }
   ],
   "source": [
    "# Print the number of single_axis and dual_axis arrays\n",
    "print(f'The number of single_axis arrays in SolarPACES in the US is {len(solarPaces[solarPaces[\"mount\"] == \"single_axis\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare GEM GSPT Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of gspt arrays where location accuracy is not exact is 54\n",
      "The number of arrays in GSPT in the US is 5524\n"
     ]
    }
   ],
   "source": [
    "# Read the second and third sheets of the excel file (first is an about page)\n",
    "gspt2 = pd.read_excel(gspt_path, sheet_name=1)  # Second sheet (<20 MW)\n",
    "gspt3 = pd.read_excel(gspt_path, sheet_name=2)  # Third sheet (>20 MW)\n",
    "\n",
    "# Combine the dataframes\n",
    "gspt = pd.concat([gspt2, gspt3], ignore_index=True)\n",
    "\n",
    "# Subset for the United States from Country/Area column\n",
    "gspt = gspt[gspt['Country/Area'] == 'United States']\n",
    "\n",
    "# Subset for Status = 'operating', Location accuracy is = 'exact', and Start year is less than or equal to mostRecentInstallYear\n",
    "gspt = gspt[gspt['Status'] == 'operating']\n",
    "print(f'The number of gspt arrays where location accuracy is not exact is {len(gspt[gspt[\"Location accuracy\"] != \"exact\"])}') # Print the number of gspt where location accuracy is not exact \n",
    "#gspt = gspt[gspt['Location accuracy'] == 'exact'] # 54 potentially lost arrays, although some may innacurate centroids of existing arrays\n",
    "gspt = gspt[gspt['Start year'] <= mostRecentInstallYear]\n",
    "\n",
    "# Create a centroid shape file from the Latitude and Longitude columns\n",
    "gspt['geometry'] = gpd.points_from_xy(gspt['Longitude'], gspt['Latitude'])\n",
    "\n",
    "# Convert to GeoDataFrame and transform to projection of USPVDB\n",
    "gspt = gpd.GeoDataFrame(gspt, geometry='geometry', crs=4326)\n",
    "gspt = gspt.to_crs(uspvdb.crs)\n",
    "\n",
    "# Mask for US boundary\n",
    "gspt = gspt[gspt.intersects(US_boundary.unary_union)]\n",
    "\n",
    "# For all rows with capacity rating of 'MWac', convert to 'MWdc' by multiplying by 1.2 (assumed 20% DC to AC ratio)\n",
    "gspt.loc[gspt['Capacity Rating'] == 'MWac', 'Capacity (MW)'] = gspt.loc[gspt['Capacity Rating'] == 'MWac', 'Capacity (MW)'] * 1.2\n",
    "\n",
    "# Set a column for agrivoltaic type and fill with NaN\n",
    "gspt['AVtype'] = np.nan\n",
    "\n",
    "# Set a column for area and fill with NaN\n",
    "gspt['area'] = np.nan\n",
    "\n",
    "# Set Technology Type as modType\n",
    "gspt['modType'] = gspt['Technology Type'].str.lower()\n",
    "\n",
    "# If modType contains 'pv', set to c-si. If modType contains 'thermal', set to csp. Else, set to c-si. \n",
    "gspt.loc[gspt['modType'].str.contains('pv', case=False, na=False), 'modType'] = 'c-si'\n",
    "gspt.loc[gspt['modType'].str.contains('thermal', case=False, na=False), 'modType'] = 'csp'\n",
    "gspt.loc[~gspt['modType'].isin(['c-si', 'csp']), 'modType'] = 'c-si'\n",
    "\n",
    "# Set an empty column for azimuth filled with -9999\n",
    "gspt['azimuth'] = -9999\n",
    "\n",
    "# Set an empty column for mount filled with NaN\n",
    "gspt['mount'] = np.nan\n",
    "gspt['mount'] = gspt['mount'].astype(object)\n",
    "\n",
    "# Format data\n",
    "gspt = formatDf(df = gspt, nativeIdentifier = 'Project Name', installationYear = 'Start year', capacityMWdc = 'Capacity (MW)', area_m2 = 'area', moduleType= 'modType', agrivoltaicType = 'AVtype', azimuth = 'azimuth', mountTechnology = 'mount', source = 'GSPT')\n",
    "\n",
    "# Print the number of arrays\n",
    "print(f'The number of arrays in GSPT in the US is {len(gspt)}')\n",
    "\n",
    "# Export\n",
    "gspt.to_file(os.path.join(derivedTemp_path, r'gspt_point.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare WRI GPPDB Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stidjaco\\AppData\\Local\\Temp\\ipykernel_13900\\4183648544.py:2: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  gppdb = pd.read_csv(gppdb_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of arrays in GPPDB in the US is 3248\n"
     ]
    }
   ],
   "source": [
    "# Call data\n",
    "gppdb = pd.read_csv(gppdb_path)\n",
    "\n",
    "# Filter for county = 'USA' and primary_fuel = 'Solar'\n",
    "gppdb = gppdb[(gppdb['country'] == 'USA') & (gppdb['primary_fuel'] == 'Solar')]\n",
    "\n",
    "# Create centroid geometry from latitute and longitude\n",
    "gppdb['geometry'] = gpd.points_from_xy(gppdb['longitude'], gppdb['latitude'])\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "gppdb = gpd.GeoDataFrame(gppdb, crs=gee_crs)\n",
    "\n",
    "# Transform to projection of USPVDB\n",
    "gppdb = gppdb.to_crs(uspvdb.crs)\n",
    "\n",
    "# Mask the gppdb dataset with the US boundary\n",
    "gppdb = gppdb[gppdb.intersects(US_boundary.unary_union)]\n",
    "\n",
    "# Print unique values for source\n",
    "# print(gppdb['source'].unique())\n",
    "\n",
    "# Set a column for agrivoltaic type and fill with NaN\n",
    "gppdb['AVtype'] = np.nan\n",
    "\n",
    "# Add modType column and fill with 'c-si'\n",
    "gppdb['modType'] = 'c-si'\n",
    "\n",
    "# Add area column and fill with -9999\n",
    "gppdb['area'] = -9999\n",
    "\n",
    "# Add azimuth column and fill with -9999\n",
    "gppdb['azimuth'] = -9999\n",
    "\n",
    "# Add mount column and fill with NaN\n",
    "gppdb['mount'] = np.nan\n",
    "gppdb['mount'] = gppdb['mount'].astype(object)\n",
    "\n",
    "# Format data (used to use gppd_idnr, but name gives more context for searches)\n",
    "gppdb = formatDf(df = gppdb, nativeIdentifier = 'name', installationYear = 'commissioning_year', capacityMWdc = 'capacity_mw', area_m2 = 'area', moduleType = 'modType', agrivoltaicType = 'AVtype', azimuth = 'azimuth', mountTechnology = 'mount', source = 'GPPDB')\n",
    "\n",
    "# Print the number of arrays\n",
    "print(f'The number of arrays in GPPDB in the US is {len(gppdb)}')\n",
    "\n",
    "# Export shapefile\n",
    "gppdb.to_file(os.path.join(derivedTemp_path, r'gppdb_point.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export InSPIRE, LBNL-USS, PV-DAQ, SolarPaces, GSPT, and GPPDB, in Need of Digitization\n",
    "Preferenced in order of perceived attribute and method derivation quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique arrays needing manual digitization:  1616  arrays\n"
     ]
    }
   ],
   "source": [
    "# Call polygon data\n",
    "existingDatasetArrayShapes = gpd.read_file(os.path.join(derivedTemp_path, r'existingDatasetArrayShapes.shp'))\n",
    "\n",
    "# Call point data\n",
    "inspire = gpd.read_file(os.path.join(derivedTemp_path, r'inspire_point.shp'))\n",
    "lnblUse = gpd.read_file(os.path.join(derivedTemp_path, r'lbnlUss_point.shp'))\n",
    "pvdaq = gpd.read_file(os.path.join(derivedTemp_path, r'pvdaq_point.shp'))\n",
    "solarPaces = gpd.read_file(os.path.join(derivedTemp_path, r'solarPaces_point.shp'))\n",
    "gspt = gpd.read_file(os.path.join(derivedTemp_path, r'gspt_point.shp'))\n",
    "gppdb = gpd.read_file(os.path.join(derivedTemp_path, r'gppdb_point.shp'))\n",
    "\n",
    "# Buffer the point data by 100 meters (overlapDist) account for potential misalignment of point data\n",
    "inspire_buffer = inspire.copy()\n",
    "inspire_buffer['geometry'] = inspire_buffer.buffer(overlapDist)\n",
    "lbnlUss_buffer = lbnlUss.copy()\n",
    "lbnlUss_buffer['geometry'] = lbnlUss_buffer.buffer(overlapDist)\n",
    "pvdaq_buffer = pvdaq.copy()\n",
    "pvdaq_buffer['geometry'] = pvdaq_buffer.buffer(overlapDist)\n",
    "solarPaces_buffer = solarPaces.copy()\n",
    "solarPaces_buffer['geometry'] = solarPaces_buffer.buffer(overlapDist)\n",
    "gspt_buffer = gspt.copy()\n",
    "gspt_buffer['geometry'] = gspt_buffer.buffer(overlapDist)\n",
    "gppdb_buffer = gppdb.copy()\n",
    "gppdb_buffer['geometry'] = gppdb_buffer.buffer(overlapDist)\n",
    "\n",
    "# Get point data that is not within 190 meters of other point data. Spatial quality is in theory the same for these, given percieved dataset quality, use the following order of priority: InSPIRE, PVDAQ, SolarPACES, GSPT, GPPDB\n",
    "inspire_unique = inspire_buffer\n",
    "lbnlUss_unique = lbnlUss_buffer[~lbnlUss_buffer.intersects(inspire_buffer.unary_union)]\n",
    "pvdaq_unique = pvdaq_buffer[~pvdaq_buffer.intersects(inspire_buffer.unary_union)]\n",
    "pvdaq_unique = pvdaq_unique[~pvdaq_unique.intersects(lbnlUss_buffer.unary_union)]\n",
    "solarPaces_unique = solarPaces_buffer[~solarPaces_buffer.intersects(inspire_buffer.unary_union)]\n",
    "solarPaces_unique = solarPaces_unique[~solarPaces_unique.intersects(lbnlUss_buffer.unary_union)]\n",
    "solarPaces_unique = solarPaces_unique[~solarPaces_unique.intersects(pvdaq_buffer.unary_union)]\n",
    "gspt_unique = gspt_buffer[~gspt_buffer.intersects(inspire_buffer.unary_union)]\n",
    "gspt_unique = gspt_unique[~gspt_unique.intersects(lbnlUss_buffer.unary_union)]\n",
    "gspt_unique = gspt_unique[~gspt_unique.intersects(pvdaq_buffer.unary_union)]\n",
    "gspt_unique = gspt_unique[~gspt_unique.intersects(solarPaces_buffer.unary_union)]\n",
    "gppdb_unique = gppdb_buffer[~gppdb_buffer.intersects(inspire_buffer.unary_union)]\n",
    "gppdb_unique = gppdb_unique[~gppdb_unique.intersects(lbnlUss_buffer.unary_union)]\n",
    "gppdb_unique = gppdb_unique[~gppdb_unique.intersects(pvdaq_buffer.unary_union)]\n",
    "gppdb_unique = gppdb_unique[~gppdb_unique.intersects(solarPaces_buffer.unary_union)]\n",
    "gppdb_unique = gppdb_unique[~gppdb_unique.intersects(gspt_buffer.unary_union)]\n",
    "\n",
    "# Merge the unique point data\n",
    "mergedPoints = gpd.GeoDataFrame(pd.concat([inspire_unique, lbnlUss_unique, pvdaq_unique, solarPaces_unique, gspt_unique, gppdb_unique], ignore_index=True), crs=uspvdb.crs)\n",
    "\n",
    "# Get points that are not within 190 meters of merged arrays\n",
    "points_unique = mergedPoints[~mergedPoints.intersects(existingDatasetArrayShapes.unary_union)]\n",
    "\n",
    "# Print the resulting number of arrays needing manual digitization\n",
    "print(\"Unique arrays needing manual digitization: \", len(points_unique), \" arrays\")\n",
    "\n",
    "# Export\n",
    "points_unique.to_file(os.path.join(derivedTemp_path, r'points_toDigitize.shp'))\n",
    "mergedPoints.to_file(os.path.join(derivedTemp_path, r'points_all.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Solar Panel Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of initial mergedPanels is 1079042\n",
      "Number of final panel-rows in existing datasets: 1076800\n",
      "Total area of panels in mergedPanels dataset is 138.13188623671547 km2\n",
      "Number of unique arrays in the mergedPanels dataset is 5087\n"
     ]
    }
   ],
   "source": [
    "# This cell requires ~200 minutes to run\n",
    "\n",
    "# Call OSM and CCVPV panel data\n",
    "osmPanels = gpd.read_file(osm_PanelsPath)\n",
    "ccvpvPanels = gpd.read_file(ccvpv_PanelsPath)\n",
    "\n",
    "# Call in OSM and CCVPV array data\n",
    "osmArrays = gpd.read_file(osm_ArraysPath)\n",
    "ccvpvArrays = gpd.read_file(ccvpv_ArraysPath)\n",
    "\n",
    "# Ensure all datasets are in the same projection as USPVDB\n",
    "osmPanels = osmPanels.to_crs(uspvdb.crs)\n",
    "osmArrays = osmArrays.to_crs(uspvdb.crs)\n",
    "ccvpvPanels = ccvpvPanels.to_crs(uspvdb.crs)\n",
    "ccvpvArrays = ccvpvArrays.to_crs(uspvdb.crs)\n",
    "\n",
    "# Drop unnecessary columns from OSM panels\n",
    "osmPanels = osmPanels.drop(columns=['arrayID', 'PnlNum', 'Source', 'ProjName', 'cap_mw', 'instYr'])\n",
    "\n",
    "# For Class column in CCVPV, if \"Si_Fixed_S\" set to 'fixed_axis\", if 'Si_Single_E/W\" set to 'single_axis'. Change class name to mount\n",
    "ccvpvPanels.loc[ccvpvPanels['Class'] == 'Si_Fixed_S', 'Class'] = 'fixed_axis'\n",
    "ccvpvPanels.loc[ccvpvPanels['Class'] == 'Si_Single_E/W', 'Class'] = 'single_axis'\n",
    "ccvpvPanels = ccvpvPanels.rename(columns={'Class': 'mount'})\n",
    "\n",
    "# Add a nativeID column to CCVPV panels that is the row index\n",
    "ccvpvPanels['nativeID'] = ccvpvPanels.index\n",
    "\n",
    "# Add a modType column to CCVPV panels that is 'c-si'\n",
    "ccvpvPanels['modType'] = 'c-si'\n",
    "\n",
    "# Calculate the area of each panel (in square meters)\n",
    "ccvpvPanels['area'] = ccvpvPanels['geometry'].apply(lambda x: x.area if x.is_valid and x.area > 0 else np.nan)\n",
    "\n",
    "# Calculate the perimeter-to-area ratio of each panel\n",
    "ccvpvPanels['PmArRatio'] = ccvpvPanels['geometry'].apply(lambda x: x.length / x.area if x.is_valid and x.area > 0 else np.nan)\n",
    "\n",
    "# Buffer osm panels by 10 meters, dissolve, and unbuffer by -10 meters to create array geometries, and remove ccvpv panels interset with osm panel arrays\n",
    "osmPanels_buffer = osmPanels.copy()\n",
    "osmPanels_buffer['geometry'] = osmPanels_buffer.buffer(panelArrayBuff)\n",
    "osmPanels_dissolved = osmPanels_buffer.dissolve()\n",
    "osmPanels_dissolved['geometry'] = osmPanels_dissolved.buffer(-panelArrayBuff)\n",
    "\n",
    "# Remove ccvpv panels that intersect with osm panel arrays. Here we prioritize OSM over CCVPV because CCVPV is derived from imagery, and because Stid et al., 2022 manually digitized row geometries that were missing row-portions, while preserving eCognition geometries (not intersecting digitized geometries).\n",
    "ccvpvPanels = ccvpvPanels[~ccvpvPanels.intersects(osmPanels_dissolved.unary_union)]\n",
    "\n",
    "# Save the panel dataset source to each dataset\n",
    "osmPanels['Source'] = 'OSM'\n",
    "ccvpvPanels['Source'] = 'CCVPV'\n",
    "\n",
    "# Merge the panel data\n",
    "mergedPanels = gpd.GeoDataFrame(pd.concat([osmPanels, ccvpvPanels], ignore_index=True), crs=uspvdb.crs)\n",
    "\n",
    "# Export (not necesssary, but the inersects above is the most intensive process [~182 minutes], so valuable to save out product)\n",
    "#mergedPanels.to_file(os.path.join(derivedTemp_path, r'mergedPanels.shp')) \n",
    "\n",
    "# Print the initial number of mergedPanels to check if we are producing duplicates in the following lines\n",
    "print(f'The number of initial mergedPanels is {len(mergedPanels)}')\n",
    "\n",
    "# Add a initID column to mergedPanels that is the row index\n",
    "mergedPanels['tempID'] = mergedPanels.index\n",
    "\n",
    "# For both osmArrays and ccvpvArrays, add an array ID column name that is the row index (as a string) + a string for the datas\n",
    "osmArrays['osmID'] = osmArrays.index.astype(str) + '_OSM'\n",
    "ccvpvArrays['ccvpvID'] = ccvpvArrays.index.astype(str) + '_CCVPV'\n",
    "\n",
    "# Copy instYr column from OSM arrays to mergedPanels using spatial join. Also grab osmID column from osmArrays\n",
    "mergedPanels = gpd.sjoin(mergedPanels, osmArrays[['instYr', 'osmID', 'geometry']], how='left', predicate='intersects')\n",
    "mergedPanels = mergedPanels.reset_index(drop=True)\n",
    "mergedPanels = mergedPanels.drop(columns=['index_left', 'index_right'], errors='ignore')\n",
    "\n",
    "# Copy Yr_inst column from CCVPV arrays to mergedPanels using spatial join and call it instYr. Also grab ccvpvID column from ccvpvArrays\n",
    "mergedPanels = gpd.sjoin(mergedPanels, ccvpvArrays[['Yr_inst', 'ccvpvID', 'geometry']], how='left', predicate='intersects')\n",
    "mergedPanels = mergedPanels.reset_index(drop=True)\n",
    "mergedPanels = mergedPanels.drop(columns=['index_left', 'index_right'], errors='ignore')\n",
    "\n",
    "# Fill instYr column with Yr_inst where instYr is NaN\n",
    "mergedPanels['instYr'] = mergedPanels['instYr'].fillna(mergedPanels['Yr_inst'])\n",
    "\n",
    "# Drop Yr_inst column\n",
    "mergedPanels = mergedPanels.drop(columns=['Yr_inst'])\n",
    "\n",
    "# The join above may have duplicated rows, so drop duplicates\n",
    "mergedPanels = mergedPanels.drop_duplicates(subset='tempID')\n",
    "\n",
    "# Drop panels with less area than the minimum panel area\n",
    "mergedPanels = mergedPanels[mergedPanels['area'] >= minPanelRowArea]\n",
    "\n",
    "# Print number of rows in mergedPanels\n",
    "print(f'Number of final panel-rows in existing datasets: {len(mergedPanels)}')\n",
    "\n",
    "# Print the total sum of 'area' in the mergedPanels dataset in km2\n",
    "print(f'Total area of panels in mergedPanels dataset is {mergedPanels[\"area\"].sum() / 1e6} km2')\n",
    "\n",
    "# Set an arrayID column. This should be osmID, and if NaN, ccvpvID. If both are NaN, keep NaN. \n",
    "# Then, we want to find the number of unique arrayID's in the mergedPanels dataset and print \n",
    "mergedPanels['arrayID'] = mergedPanels['osmID'].fillna(mergedPanels['ccvpvID'])\n",
    "print(f'Number of unique arrays in the mergedPanels dataset is {len(mergedPanels[\"arrayID\"].unique())}')\n",
    "\n",
    "# Drop tempID that is the row index for the mergedPanels dataset, and arrayID columns\n",
    "mergedPanels = mergedPanels.reset_index(drop=True)\n",
    "mergedPanels = mergedPanels.drop(columns=['tempID', 'arrayID', 'osmID', 'ccvpvID'])\n",
    "mergedPanels['panelID'] = mergedPanels.index\n",
    "\n",
    "# Export\n",
    "mergedPanels.to_file(os.path.join(derivedTemp_path, r'existingDatasetPanelShapes.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Panel Area Percentiles for CCVPV and OSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum panel area is 0.004586387346827453\n",
      "The 1st percentile panel area is 14.638318945940224\n",
      "The 5th percentile panel area is 27.900074620621066\n",
      "The 95th percentile panel area is 254.17212304611706\n",
      "The 99th percentile panel area is 449.9727782791575\n",
      "The maximum panel area is 1310.7696045794355\n",
      "The minimum panel area is 15.000837364512863\n",
      "The 1st percentile panel area is 15.204260869212009\n",
      "The 5th percentile panel area is 15.329141866298716\n",
      "The 95th percentile panel area is 407.12461017643625\n",
      "The 99th percentile panel area is 866.9789413470319\n",
      "The maximum panel area is 1980.0579333428238\n"
     ]
    }
   ],
   "source": [
    "# Call ccvpv panels, transform to projection of USPVDB, calculate area, and print the min, 1st, 95th, 99th, and max percentile panel area\n",
    "ccvpvPanels = gpd.read_file(ccvpv_PanelsPath)\n",
    "ccvpvPanels = ccvpvPanels.to_crs(uspvdb.crs)\n",
    "ccvpvPanels['Pnl_a'] = ccvpvPanels['geometry'].area\n",
    "print(f'The minimum panel area is {ccvpvPanels[\"Pnl_a\"].min()}')\n",
    "print(f'The 1st percentile panel area is {ccvpvPanels[\"Pnl_a\"].quantile(0.01)}')\n",
    "print(f'The 5th percentile panel area is {ccvpvPanels[\"Pnl_a\"].quantile(0.05)}') \n",
    "print(f'The 95th percentile panel area is {ccvpvPanels[\"Pnl_a\"].quantile(0.95)}')\n",
    "print(f'The 99th percentile panel area is {ccvpvPanels[\"Pnl_a\"].quantile(0.99)}')\n",
    "print(f'The maximum panel area is {ccvpvPanels[\"Pnl_a\"].max()}')\n",
    "\n",
    "# Now do the same for the OSM panels\n",
    "osmPanels = gpd.read_file(osm_PanelsPath)\n",
    "osmPanels = osmPanels.to_crs(uspvdb.crs)\n",
    "osmPanels['Pnl_a'] = osmPanels['geometry'].area\n",
    "print(f'The minimum panel area is {osmPanels[\"Pnl_a\"].min()}')\n",
    "print(f'The 1st percentile panel area is {osmPanels[\"Pnl_a\"].quantile(0.01)}')\n",
    "print(f'The 5th percentile panel area is {osmPanels[\"Pnl_a\"].quantile(0.05)}')\n",
    "print(f'The 95th percentile panel area is {osmPanels[\"Pnl_a\"].quantile(0.95)}')\n",
    "print(f'The 99th percentile panel area is {osmPanels[\"Pnl_a\"].quantile(0.99)}')\n",
    "print(f'The maximum panel area is {osmPanels[\"Pnl_a\"].max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END: Digitize Missing Point Location Rough Array Bounds with `script2_digitizeSolarArrays`, call and combine with existing arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*END*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum area of panels in the existingDatasetPanelShapes dataset is 0.004586387346827 m2\n",
      "The 5th percentile area of panels in the existingDatasetPanelShapes dataset is 35.286073053263934 m2\n"
     ]
    }
   ],
   "source": [
    "# Call exisitng dataset panel data\n",
    "existingDatasetPanelShapes = gpd.read_file(os.path.join(derivedTemp_path, r'existingDatasetPanelShapes.shp'))\n",
    "\n",
    "# Print the minimum area and the 5th percentile area of the existingDatasetPanelShapes dataset\n",
    "print(f'The minimum area of panels in the existingDatasetPanelShapes dataset is {existingDatasetPanelShapes[\"area\"].min()} m2')\n",
    "print(f'The 5th percentile area of panels in the existingDatasetPanelShapes dataset is {existingDatasetPanelShapes[\"area\"].quantile(0.01)} m2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5th percentile area of panels in the existingDatasetPanelShapes dataset is 1245.953201627507 m2\n"
     ]
    }
   ],
   "source": [
    "print(f'The 5th percentile area of panels in the existingDatasetPanelShapes dataset is {existingDatasetPanelShapes[\"area\"].quantile(0.999)} m2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigPanel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
